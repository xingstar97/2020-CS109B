{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109B Advanced Data Science\n",
    "## Lab 8: Recurrent Neural Networks\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2020**<br>\n",
    "**Instructors:** Mark Glickman, Pavlos Protopapas, and Chris Tanner<br>\n",
    "**Lab Instructors:** Chris Tanner and Eleni Angelaki Kaxiras<br>\n",
    "**Content:** Srivatsan Srinivasan, Pavlos Protopapas, Chris Tanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.discussion {\n",
       "\tbackground-color: #ccffcc;\n",
       "\tborder-color: #88E97A;\n",
       "\tborder-left: 5px solid #0A8000; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "In this lab we will look at Recurrent Neural Networks (RNNs), LSTMs and their building blocks.\n",
    "\n",
    "By the end of this lab, you should:\n",
    "\n",
    "- know how to put together the building blocks used in RNNs and its variants (GRU, LSTM) in `keras` with an example.\n",
    "- have a good undertanding on how sequences -- any data that has some temporal semantics (e.g., time series, natural language, images etc.) -- fit into and benefit from a recurrent architecture\n",
    "- be familiar with preprocessing text and dynamic embeddings\n",
    "- be familiar with gradient issues on RNNs processing longer sentence lengths\n",
    "- understand different kinds of LSTM architectures, classifiers, sequence to sequence models and their far-reaching applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMDb Review Classification: Feedforward, CNN, RNN, LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we are going to do sentiment classification on a movie review dataset. We are going to build a feedforward net, a convolutional neural net, a recurrent net and combine one or more of them to understand performance of each of them. A sentence can be thought of as a sequence of words that collectively represent meaning. Individual words impact the meaning. Thus, the context matters; words that occur earlier in the sentence influence the sentence's structure and meaning in the latter part of the sentence (e.g., Jose asked Anqi if she were going to the library today). Likewise, words that occur later in a sentence can affect the meaning of earlier words (e.g., Apple is an interesting company). As we have seen in lecture, if we wish to make use of a full sentence's context in both directions, then we should use a bi-directional RNN (e.g., Bi-LSTM). For the purpose of this tutorial, we are going to restrict ourselves to only uni-directional RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:39:51.590167: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-21 14:39:51.613692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 14:39:51.997645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "# from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "# from keras.layers.convolutional import Conv1D\n",
    "# from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# We also want to have a finite length of reviews and not have to process really long sentences.\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers have no built-in knowledge of words or their meanings and cannot understand them in any rich way that humans do -- hence, the purpose of Natural Language Processing (NLP). As with any data science, computer science, machine learning task, the first crucial step is to clean (pre-process) your data so that you can soundly make use of it. Within NLP, this first step is called Tokenization and it concerns how to represent each token (a.k.a. word) of your corpus (i.e., dataset).\n",
    "\n",
    "#### TOKENIZATION\n",
    "\n",
    "A ``token`` refers to a single, atomic unit of meaning (i.e., a word). How should our computers represent each word? We could read in our corpus word by word and store each word as a String (data structure). However, Strings tend to use more computer memory than Integers and can become cumbersome. As long as we preserve the uniqueness of the tokens and are consistent, we are better off converting each distinct word to a distinct number (Integer). This is standard practice within NLP / computer science / data science, etc.\n",
    "As a simple example of tokenization, we can see a small example.\n",
    "\n",
    "If the five sentences below were our entire corpus, our conversion would look as follows:\n",
    "\n",
    "1. i have books - [1, 4, 7]\n",
    "2. interesting books are useful [10,2,9,8]\n",
    "3. i have computers [1,4,6]\n",
    "4. computers are interesting and useful [6,9,11,10,8]\n",
    "5. books and computers are both valuable. [2,10,2,9,13,12]\n",
    "6. Bye Bye [7,7]\n",
    "\n",
    "Create tokens for vocabulary based on frequency of occurrence. Hence, we assign the following tokens\n",
    "\n",
    "I-1, books-2, computers-3, have-4, are-5, computers-6,bye-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
    "\n",
    "Thankfully, our dataset is already represented in such a tokenized form.\n",
    "\n",
    "**NOTE:** Often times, depending on your NLP task, it is useful to also perform other pre-processing, cleaning steps, such as:\n",
    "- treating each punctuation mark as a token (e.g., , . ! ? are each separate tokens)\n",
    "- lower-casing all words (so that a given word isn't treated differently just because it starts a sentence or not)\n",
    "- separating each sentence with a unique symbol (e.g., <S> and </S>)\n",
    "- removing words that are incredibly common (e.g., function words, (in)definite articles). These are referred to as 'stopwords'). For language modelling, we DO NOT remove stopwords. A sentence's meaning needs to include all of the original words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n",
      "Number of reviews 25000\n",
      "Length of first and fifth review before padding 218 147\n",
      "First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First label 1\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print('Number of reviews', len(X_train))\n",
    "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
    "print('First review', X_train[0])\n",
    "print('First label', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data\n",
    "\n",
    "If we were training our RNN one sentence at a time, it would be okay to have sentences of varying lengths. However, as with any neural network, it can be sometimes be advantageous to train inputs in batches. When doing so with RNNs, our input tensors need to be of the same length/dimensions. Thus, let's pad our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first and fifth review after padding 500 500\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1A : FEED-FORWARD NETWORKS WITHOUT EMBEDDINGS \n",
    "\n",
    "Let us build a single-layer feed-forward net with a hidden layer of 250 nodes. Each input would be a 500-dim vector of tokens since we padded all our sequences to size 500.\n",
    "\n",
    "<br>\n",
    "<div class=\"exercise\"  style=\"background-color:#b3e6ff\">\n",
    "<b>EXERCISE</b>: Calculate the number of parameters involved in this network and implement a feedforward net to do classification without looking at cells below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:40:03.295035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:03.385805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:03.386086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:03.388311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:03.388514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:03.388639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:04.801364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:04.801468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:04.801519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 14:40:04.801583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9238 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 250)               125250    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125501 (490.24 KB)\n",
      "Trainable params: 125501 (490.24 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:40:07.566698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-21 14:40:07.583894: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe9751c6150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-21 14:40:07.583907: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2023-09-21 14:40:07.607856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-21 14:40:07.870688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-21 14:40:07.930689: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-21 14:40:07.999245: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 - 3s - loss: 177.3750 - accuracy: 0.5039 - val_loss: 90.6364 - val_accuracy: 0.5073 - 3s/epoch - 15ms/step\n",
      "Epoch 2/10\n",
      "196/196 - 1s - loss: 45.8141 - accuracy: 0.5786 - val_loss: 44.9000 - val_accuracy: 0.5039 - 1s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "196/196 - 1s - loss: 15.7880 - accuracy: 0.6554 - val_loss: 26.4167 - val_accuracy: 0.5067 - 951ms/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "196/196 - 1s - loss: 6.4456 - accuracy: 0.7210 - val_loss: 17.6518 - val_accuracy: 0.5088 - 1s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "196/196 - 1s - loss: 3.1993 - accuracy: 0.7581 - val_loss: 12.1551 - val_accuracy: 0.5104 - 1s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "196/196 - 1s - loss: 1.9322 - accuracy: 0.7785 - val_loss: 9.6879 - val_accuracy: 0.5045 - 1s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "196/196 - 1s - loss: 1.3275 - accuracy: 0.7994 - val_loss: 7.5593 - val_accuracy: 0.5114 - 1s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "196/196 - 1s - loss: 0.9882 - accuracy: 0.8155 - val_loss: 7.0871 - val_accuracy: 0.5091 - 1s/epoch - 7ms/step\n",
      "Epoch 9/10\n",
      "196/196 - 1s - loss: 0.8899 - accuracy: 0.8237 - val_loss: 6.7064 - val_accuracy: 0.5098 - 988ms/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "196/196 - 1s - loss: 0.7921 - accuracy: 0.8317 - val_loss: 6.8316 - val_accuracy: 0.5101 - 1s/epoch - 5ms/step\n",
      "Accuracy: 51.00%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(250, activation='relu',input_dim=max_review_length))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"  style=\"background-color:#F5E4C3\">\n",
    "    <b>Discussion:</b> Why was the performance bad? What was wrong with tokenization?\n",
    "</div>\n",
    "\n",
    "### MODEL 1B : FEED-FORWARD NETWORKS WITH EMBEDDINGS\n",
    "\n",
    "#### What is an embedding layer ? \n",
    "\n",
    "An embedding is a \"distributed representation\" (e.g., vector) of a particular atomic item (e.g., word token, object, etc). When representing items by embeddings:\n",
    "- each distinct item should be represented by its own unique embedding\n",
    "- the semantic similarity between items should correspond to the similarity between their respective embeddings (i.e., words that are more similar to one another should have embeddings that are more similar to each other).\n",
    "\n",
    "There are essentially an infinite number of ways to create such embeddings, and since these representations have such a great influence on the performance of our models, there has been an incredible amount of research dedicated to this very aspect. If you are interested in learning more, start with the astromonically impactful papers of [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) and [GloVe](https://www.aclweb.org/anthology/D14-1162.pdf).\n",
    "\n",
    "In general, though, one can view the embedding process as a linear projection from one vector space to another (e.g., a vector space of unique words being mapped to a world of fixed-length, dense vectors filled with continuous-valued numbers. For NLP, we usually use embeddings to project the one-hot encodings of words (i.e., a vector that is the length of the entire vocabulary, and it is filled with all zeros except for a single value of 1 that corresponds to the particular word) on to a lower-dimensional continuous space (e.g., vectors of size 100) so that the input surface is dense and possibly smooth. Thus, one can view this embedding layer process as just a transformation from $\\mathbb{R}^{inp}$ to $\\mathbb{R}^{emb}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create an Embedding layer, TensorFlow initializes an embedding matrix with random values. The shape of this matrix is (input_dim, output_dim), where input_dim is the size of the input vocabulary, and output_dim is the dimension of the embedding vectors. Each row in this matrix corresponds to an index in the input vocabulary.\n",
    "\n",
    "Input Transformation: When you pass integer sequences to the embedding layer, it essentially looks up the corresponding rows in the embedding matrix. If the input is a sequence of integer indices [3, 7, 2], the embedding layer will retrieve the rows with indices 3, 7, and 2 from the embedding matrix. These rows are the dense embeddings for the input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 250)               12500250  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13500501 (51.50 MB)\n",
      "Trainable params: 13500501 (51.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# inputs will be converted from batch_size * sentence_length to batch_size*sentence_length*embedding _dim\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 8s - loss: 0.0136 - accuracy: 0.9979 - val_loss: 0.4066 - val_accuracy: 0.8696 - 8s/epoch - 39ms/step\n",
      "Epoch 2/2\n",
      "196/196 - 6s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4620 - val_accuracy: 0.8716 - 6s/epoch - 30ms/step\n",
      "Accuracy: 87.16%\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 : CONVOLUTIONAL NEURAL NETWORKS (CNN)\n",
    "Text can be thought of as 1-dimensional sequence (a single, long vector) and we can apply 1D Convolutions over a set of word embeddings. Let us walk through convolutions on text data with [this blog](http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/). If you want to learn more, read this [published and well-cited paper](https://www.aclweb.org/anthology/I17-1026.pdf) from my friend, Byron Wallace.\n",
    "\n",
    "Fit a 1D convolution with 200 filters, kernel size 3, followed by a feed-forward layer of 250 nodes, and ReLU and Sigmoid activations as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 200)          60200     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 250, 200)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50000)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 250)               12500250  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13560701 (51.73 MB)\n",
      "Trainable params: 13560701 (51.73 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 14:42:34.537747: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 16s 66ms/step - loss: 0.4951 - accuracy: 0.7290\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 10s 53ms/step - loss: 0.1877 - accuracy: 0.9277\n",
      "Accuracy: 87.80%\n"
     ]
    }
   ],
   "source": [
    "# %load sol2.py\n",
    "# create the CNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Convolution1D(filters=200, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# train the CNN\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=128)\n",
    "\n",
    "# evalute the CNN\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3 : Simple RNN\n",
    "\n",
    "Two great blogs that are helpful for understanding the workings of a RNN and LSTM are\n",
    "\n",
    "1. http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "At a high-level, an RNN is similar to a feed-forward neural network (FFNN) in that there is an input layer, a hidden layer, and an output layer. The input layer is fully connected to the hidden layer, and the hidden layer is fully connected to the output layer. However, the crux of what makes it a **recurrent** neural network is that the hidden layer for a given time _t_ is not only based on the input layer at time _t_ but also the hidden layer from time _t-1_.\n",
    "\n",
    "Mathematically, a simpleRNN can be defined by the following recurrence relation.\n",
    "\n",
    "<center>$h_t = \\sigma(W([h_{t-1},x_{t}])+b)$\n",
    "    \n",
    "If we extend this recurrence relation to the length of sequences we have in hand, our RNN architecture can be viewed as follows (this is also referred to as 'unrolling' the network):\n",
    "\n",
    "<img src=\"files/fig/LSTM_classification.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1020201 (3.89 MB)\n",
      "Trainable params: 1020201 (3.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 165s 105ms/step - loss: 0.6101 - accuracy: 0.6525\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 125s 80ms/step - loss: 0.4694 - accuracy: 0.7768\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 120s 77ms/step - loss: 0.4839 - accuracy: 0.7654\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 117s 75ms/step - loss: 0.5033 - accuracy: 0.7513\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 115s 73ms/step - loss: 0.5464 - accuracy: 0.7129\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 117s 75ms/step - loss: 0.5130 - accuracy: 0.7408\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 117s 75ms/step - loss: 0.4724 - accuracy: 0.7691\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 0.4709 - accuracy: 0.7716\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 0.4894 - accuracy: 0.7573\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.4967 - accuracy: 0.7437\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 118s 75ms/step - loss: 0.4380 - accuracy: 0.7919\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 118s 76ms/step - loss: 0.4526 - accuracy: 0.7824\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 119s 76ms/step - loss: 0.3922 - accuracy: 0.8208\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 115s 74ms/step - loss: 0.4250 - accuracy: 0.7967\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 118s 76ms/step - loss: 0.4365 - accuracy: 0.7844\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 118s 75ms/step - loss: 0.3741 - accuracy: 0.8288\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 118s 76ms/step - loss: 0.3348 - accuracy: 0.8533\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 118s 75ms/step - loss: 0.3346 - accuracy: 0.8589\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 116s 74ms/step - loss: 0.3353 - accuracy: 0.8544\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 118s 76ms/step - loss: 0.3440 - accuracy: 0.8406\n",
      "Accuracy: 71.32%\n"
     ]
    }
   ],
   "source": [
    "# %load sol3.py\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(SimpleRNN(100))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNNs and vanishing/exploding gradients\n",
    "\n",
    "Let us use sigmoid activations as example. Derivative of a sigmoid can be written as \n",
    "<center> $\\sigma'(x) = \\sigma(x) \\cdot \\sigma(1-x)$. </center>\n",
    "\n",
    "<img src = \"files/fig/vanishing_gradients.png\">\n",
    "<br>\n",
    "\n",
    "Remember, an RNN is a \"really deep\" feed-forward-esque network (when unrolled in time). Hence, backpropagation happens from $h_t$ all the way to $h_1$. Also realize that sigmoid gradients are multiplicatively dependent on the value of sigmoid. Hence, if the non-activated output of any layer $h_l$ is < 0, then $\\sigma$ tends to 0, effectively \"vanishing\" the gradient. Any layer that the current layer backprops to $H_{1:L-1}$ does not learn anything useful out of the gradients.\n",
    "\n",
    "#### LSTMs and GRU\n",
    "LSTM and GRU are two sophisticated implementations of RNNs that have gates (one could say that their success hinges on using gates). A gate emits probability between 0 and 1. For instance, LSTM is built on these state updates:\n",
    "\n",
    "$L$ is a linear transformation $L(x) = W*x + b.$\n",
    "\n",
    "$f_t = \\sigma(L([h_{t-1},x_t))$\n",
    "\n",
    "$i_t = \\sigma(L([h_{t-1},x_t))$\n",
    "\n",
    "$o_t = \\sigma(L([h_{t-1},x_t))$\n",
    "\n",
    "$\\hat{C}_t = \\tanh(L([h_{t-1},x_t))$\n",
    "\n",
    "$C_t = f_t * C_{t-1}+i_t*\\hat{C}_t$  (Using the forget gate, the neural network can learn to control how much information it has to retain or forget)\n",
    "\n",
    "$h_t = o_t * \\tanh(c_t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 4 : LSTM\n",
    "\n",
    "Now, let's use an LSTM model to do classification! To make it a fair comparison to the SimpleRNN, let's start with the same architecture hyper-parameters (e.g., number of hidden nodes, epochs, and batch size). Then, experiment with increasing the number of nodes, stacking multiple layers, applying dropouts etc. Check the number of parameters that this model entails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1080501 (4.12 MB)\n",
      "Trainable params: 1080501 (4.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 38s 92ms/step - loss: 0.4244 - accuracy: 0.8017\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3684 - accuracy: 0.8363\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 0.3319 - accuracy: 0.8543\n",
      "Accuracy: 80.94%\n"
     ]
    }
   ],
   "source": [
    "# %load sol4.py\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# evaluation of the LSTM's performance\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 5 : CNN + LSTM \n",
    "\n",
    "CNNs are good at learning spatial features, and sentences can be thought of as 1-D spatial vectors (dimensionality is determined by the number of words in the sentence). We apply an LSTM over the features learned by the CNN (after a maxpooling layer). This leverages the power of CNNs and LSTMs combined! We expect the CNN to be able to pick out invariant features across the 1-D spatial structure (i.e., sentence) that characterize good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer, and the final classification can be made via a feed-forward connection to a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 500, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 500, 32)           9632      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 250, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1062933 (4.05 MB)\n",
      "Trainable params: 1062933 (4.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 27s 65ms/step - loss: 0.3890 - accuracy: 0.8106\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.2130 - accuracy: 0.9196\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.1483 - accuracy: 0.9478\n",
      "Accuracy: 87.88%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Convolution1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION\n",
    "\n",
    "We saw the power of sequence models and how they are useful in text classification. They give a solid performance, low memory footprint (thanks to shared parameters) and are able to understand and leverage the temporally connected information contained in the inputs. There is still an open debate about the performance vs memory benefits of CNNs vs RNNs in the research community.\n",
    "\n",
    "As a side-note and bit of history: how amazing is it that we can construct these neural networks, train them, and evaluate them in just a few lines of code?! Imagine if we didn't have deep learning libraries like Keras and Tensorflow; we'd have to write backpropagation and gradient descent by hand. Our last network could easily require thousands of lines of code and many hours of debugging. This is what many people did just 8 years ago, since deep learning wasn't common and the community hadn't yet written nicely packaged libraries. Many libraries have come and gone, but nowadays most people use either Tensorflow/Keras (by Google) or PyTorch (by Facebook). I expect them to remain as great libraries for the foreseeable future, so if you're interested in deep learning, it's worth the investment to learn one, or both, of them well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 231+432 = 665.... It's not ? Let's ask our LSTM\n",
    "\n",
    "In this exercise, we are going to teach addition to our model. Given two numbers (<999), the model outputs their sum (<9999). The input is provided as a string '231+432' and the model will provide its output as ' 663' (Here the empty space is the padding character). We are not going to use any external dataset and are going to construct our own dataset for this exercise.\n",
    "\n",
    "The exercise we attempt to do effectively \"translates\" a sequence of characters '231+432' to another sequence of characters ' 663' and hence, this class of models are called sequence-to-sequence models (aka seq2seq). Such architectures have profound applications in several real-life tasks such as machine translation, summarization, image captioning etc.\n",
    "\n",
    "To be clear, sequence-to-sequence (aka seq2seq) models take as input a sequence of length N and return a sequence of length M, where N and M may or may not differ, and every single observation/input may be of different values, too. For example, machine translation concerns converting text from one natural language to another (e.g., translating English to French). Google Translate is an example, and their system is a seq2seq model. The input (e.g., an English sentence) can be of any length, and the output (e.g., a French sentence) may be of any length.\n",
    "\n",
    "**Background knowledge:** The earliest and most simple seq2seq model works by having one RNN for the input, just like we've always done, and we refer to it as being an \"encoder.\" The final hidden state of the encoder RNN is fed as input to another RNN that we refer to as the \"decoder.\" The job of the decoder is to generate each token, one word at a time. This may seem really limiting, as it relies on the encoder encapsulating the entire input sequence with just 1 hidden layer. It seems unrealistic that we could encode an entire meaning of a sentence with just one hidden layer. Yet, results even in this simplistic manner can be quite impressive. In fact, these early results were compelling enough that these models immediately replaced the decades of earlier machine translation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 20:40:41.429050: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-21 20:40:41.453282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 20:40:41.886665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, RepeatVector, TimeDistributed\n",
    "import numpy as np\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The less interesting data generation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    # In Python 2, the object argument was often used when defining classes to indicate that this class is a new-style class. \n",
    "    # In Python 3 and later, this argument is optional, so you can define classes without specifying object.\n",
    "    def __init__(self, chars):        \n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    # converts a String of characters into a one-hot embedding/vector\n",
    "    def encode(self, C, num_rows):        \n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    # converts a one-hot embedding/vector into a String of characters\n",
    "    def decode(self, x, calc_argmax=True):        \n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "MAXOUTPUTLEN = DIGITS + 1\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_digit():\n",
    "  return np.random.choice(list('0123456789'))  \n",
    "\n",
    "# generate a new number of length `DIGITS`\n",
    "def generate_number():\n",
    "  num_digits = np.random.randint(1, DIGITS + 1)  \n",
    "  return int(''.join( return_random_digit()\n",
    "                      for i in range(num_digits)))\n",
    "\n",
    "# generate `TRAINING_SIZE` # of pairs of random numbers\n",
    "def data_generate(num_examples):\n",
    "  questions = []\n",
    "  answers = []\n",
    "  seen = set()\n",
    "  print('Generating data...')\n",
    "  while len(questions) < TRAINING_SIZE:      \n",
    "      a, b = generate_number(), generate_number()\n",
    "        \n",
    "      # don't allow duplicates; this is good practice for training,\n",
    "      # as we will minimize memorizing seen examples\n",
    "      key = tuple(sorted((a, b)))\n",
    "      if key in seen:\n",
    "          continue\n",
    "      seen.add(key)\n",
    "    \n",
    "      # pad the data with spaces so that the length is always MAXLEN.\n",
    "      q = '{}+{}'.format(a, b)\n",
    "      query = q + ' ' * (MAXLEN - len(q))\n",
    "      ans = str(a + b)\n",
    "    \n",
    "      # answers can be of maximum size DIGITS + 1.\n",
    "      ans += ' ' * (MAXOUTPUTLEN - len(ans))\n",
    "      questions.append(query)\n",
    "      answers.append(ans)\n",
    "  print('Total addition questions:', len(questions))\n",
    "  return questions, answers\n",
    "\n",
    "def encode_examples(questions, answers):\n",
    "  x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool_)\n",
    "  y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool_)\n",
    "  for i, sentence in enumerate(questions):\n",
    "      x[i] = ctable.encode(sentence, MAXLEN)\n",
    "  for i, sentence in enumerate(answers):\n",
    "      y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "  indices = np.arange(len(y))\n",
    "  np.random.shuffle(indices)\n",
    "  return x[indices],y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Training Data shape:\n",
      "X :  (45000, 7, 12)\n",
      "Y :  (45000, 4, 12)\n",
      "Sample Question(in encoded form) :  [[False False False False False False False  True False False False False]\n",
      " [False False False False False False False False False False  True False]\n",
      " [False False False False False False False False False False  True False]\n",
      " [False  True False False False False False False False False False False]\n",
      " [False False False  True False False False False False False False False]\n",
      " [False False False False False False False False False  True False False]\n",
      " [False False False False False  True False False False False False False]] [[False False False False False False False False False  True False False]\n",
      " [False False False False False False False False  True False False False]\n",
      " [False False False  True False False False False False False False False]\n",
      " [ True False False False False False False False False False False False]]\n",
      "Sample Question(in decoded form) :  588+173 Sample Output :  761 \n"
     ]
    }
   ],
   "source": [
    "q,a = data_generate(TRAINING_SIZE)\n",
    "x,y = encode_examples(q,a)\n",
    "\n",
    "# divides our data into training and validation\n",
    "split_at = len(x) - len(x) // 10\n",
    "x_train, x_val, y_train, y_val = x[:split_at], x[split_at:],y[:split_at],y[split_at:]\n",
    "\n",
    "print('Training Data shape:')\n",
    "print('X : ', x_train.shape)\n",
    "print('Y : ', y_train.shape)\n",
    "\n",
    "print('Sample Question(in encoded form) : ', x_train[0], y_train[0])\n",
    "print('Sample Question(in decoded form) : ', ctable.decode(x_train[0]),'Sample Output : ', ctable.decode(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's learn two wrapper functions in Keras - TimeDistributed and RepeatVector with some dummy examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TimeDistributed** is a wrapper function call that applies an input operation on all the timesteps of an input data. For instance, if I have a feed-forward network which converts a 10-dim vector to a 5-dim vector, then wrapping this TimeDistributed layer on that feed-forward operation would convert a batch_size  \\* sentence_len \\* vector_len(=10) to batch_size  \\* sentence_len \\*  output_len(=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 20:40:49.935467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:49.953511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:49.953867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:49.954843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:49.954921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:49.954967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:50.302417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:50.302519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:50.302566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-21 20:40:50.302612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9017 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input :  (1, 3, 5)\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "Shape of output :  (1, 3, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 20:40:51.069736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Inputs to it will be batch_size*time_steps*input_vector_dim(to Dense)\n",
    "# Output will be batch_size*time_steps* output_vector_dim\n",
    "# Here, Dense() converts a 5-dim input vector to a 8-dim vector.\n",
    "model.add(TimeDistributed(Dense(8), input_shape=(3, 5)))\n",
    "input_array = np.random.randint(10, size=(1,3,5))\n",
    "print(\"Shape of input : \", input_array.shape)\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(\"Shape of output : \", output_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RepeatVector** repeats the vector a specified number of times. Dimension changes from batch_size * number of elements to batch_size* number of repetitions * number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 6)\n",
      "(None, 3, 6)\n",
      "Shape of input :  (1, 10)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Shape of output :  (1, 3, 6)\n",
      "Input :  [538 688 666 152 593  47 514 640 343 146]\n",
      "Output :  [[  698.6417    214.78064  -460.49487  -473.46393 -1334.4592   -334.55194]\n",
      " [  698.6417    214.78064  -460.49487  -473.46393 -1334.4592   -334.55194]\n",
      " [  698.6417    214.78064  -460.49487  -473.46393 -1334.4592   -334.55194]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# converts from 1*10 to 1*6\n",
    "model.add(Dense(6, input_dim=10))\n",
    "print(model.output_shape)\n",
    "\n",
    "# converts from 1*6 to 1*3*6\n",
    "model.add(RepeatVector(3))\n",
    "print(model.output_shape) \n",
    "\n",
    "input_array = np.random.randint(1000, size=(1, 10))\n",
    "print(\"Shape of input : \", input_array.shape)\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "\n",
    "print(\"Shape of output : \", output_array.shape)\n",
    "# note: `None` is the batch dimension\n",
    "print('Input : ', input_array[0])\n",
    "print('Output : ', output_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE\n",
    "\n",
    "<img src=\"files/fig/LSTM_addition.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Whenever you are initializing a LSTM in Keras, by the default the option `return_sequences = False`. This means that at the end of the step the next component will only get to see the final hidden layer's values. On the other hand, if you set `return_sequences = True`, the LSTM component will return the hidden layer at each time step. It means that the next component should be able to consume inputs in that form. \n",
    "\n",
    "Think how this statement is relevant in terms of this model architecture and the TimeDistributed module we just learned.\n",
    "\n",
    "Build an encoder and decoder both single layer 128 nodes and an appropriate dense layer as needed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               72192     \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVec  (None, 4, 128)            0         \n",
      " tor)                                                            \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 4, 128)            131584    \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 4, 12)             1548      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205324 (802.05 KB)\n",
      "Trainable params: 205324 (802.05 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Hyperaparams\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "#ENCODING\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(RepeatVector(MAXOUTPUTLEN))\n",
    "# 4 outputs, each output will see the encoder hidden layers (the same layer)\n",
    "# 4 outputs are like the sequence inputs for the decoder\n",
    "\n",
    "#DECODING\n",
    "for _ in range(LAYERS):\n",
    "    # return hidden layer at each time step\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True)) \n",
    "\n",
    "model.add(TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, to_file='generator_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well our model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[np.array([0])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_val[np.array([0])], verbose=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 2, 9, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_val[np.array([0])], verbose=0).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "352/352 [==============================] - 3s 10ms/step - loss: 0.8493 - accuracy: 0.6801 - val_loss: 0.8020 - val_accuracy: 0.6931\n",
      "Finished iteration  1\n",
      "Question 28+823  True 851  Guess 850  Fail\n",
      "Question 32+12   True 44   Guess 44   Good job\n",
      "Question 17+266  True 283  Guess 284  Fail\n",
      "Question 260+79  True 339  Guess 359  Fail\n",
      "Question 603+4   True 607  Guess 607  Good job\n",
      "Question 91+693  True 784  Guess 776  Fail\n",
      "Question 39+521  True 560  Guess 560  Good job\n",
      "Question 422+280 True 702  Guess 729  Fail\n",
      "Question 569+686 True 1255 Guess 1367 Fail\n",
      "Question 3+460   True 463  Guess 464  Fail\n",
      "Question 648+9   True 657  Guess 657  Good job\n",
      "Question 850+416 True 1266 Guess 1265 Fail\n",
      "Question 938+45  True 983  Guess 983  Good job\n",
      "Question 774+40  True 814  Guess 807  Fail\n",
      "Question 4+836   True 840  Guess 830  Fail\n",
      "Question 583+661 True 1244 Guess 1225 Fail\n",
      "Question 68+820  True 888  Guess 899  Fail\n",
      "Question 27+634  True 661  Guess 660  Fail\n",
      "Question 36+64   True 100  Guess 900  Fail\n",
      "Question 358+12  True 370  Guess 379  Fail\n",
      "The model scored  25.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.7135 - accuracy: 0.7372 - val_loss: 0.6622 - val_accuracy: 0.7561\n",
      "Finished iteration  2\n",
      "Question 78+64   True 142  Guess 142  Good job\n",
      "Question 54+521  True 575  Guess 576  Fail\n",
      "Question 406+787 True 1193 Guess 1144 Fail\n",
      "Question 2+778   True 780  Guess 781  Fail\n",
      "Question 65+7    True 72   Guess 73   Fail\n",
      "Question 927+793 True 1720 Guess 1779 Fail\n",
      "Question 246+2   True 248  Guess 248  Good job\n",
      "Question 77+74   True 151  Guess 151  Good job\n",
      "Question 0+880   True 880  Guess 880  Good job\n",
      "Question 76+38   True 114  Guess 114  Good job\n",
      "Question 83+183  True 266  Guess 266  Good job\n",
      "Question 704+385 True 1089 Guess 1001 Fail\n",
      "Question 671+24  True 695  Guess 695  Good job\n",
      "Question 714+5   True 719  Guess 710  Fail\n",
      "Question 79+184  True 263  Guess 254  Fail\n",
      "Question 634+55  True 689  Guess 690  Fail\n",
      "Question 752+17  True 769  Guess 771  Fail\n",
      "Question 454+937 True 1391 Guess 1300 Fail\n",
      "Question 961+3   True 964  Guess 964  Good job\n",
      "Question 967+658 True 1625 Guess 1634 Fail\n",
      "The model scored  40.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 0.5934 - accuracy: 0.7896 - val_loss: 0.5574 - val_accuracy: 0.8000\n",
      "Finished iteration  3\n",
      "Question 93+77   True 170  Guess 161  Fail\n",
      "Question 228+18  True 246  Guess 246  Good job\n",
      "Question 441+46  True 487  Guess 486  Fail\n",
      "Question 18+61   True 79   Guess 88   Fail\n",
      "Question 496+325 True 821  Guess 800  Fail\n",
      "Question 21+94   True 115  Guess 115  Good job\n",
      "Question 612+262 True 874  Guess 754  Fail\n",
      "Question 589+656 True 1245 Guess 1244 Fail\n",
      "Question 19+51   True 70   Guess 70   Good job\n",
      "Question 120+3   True 123  Guess 123  Good job\n",
      "Question 865+95  True 960  Guess 960  Good job\n",
      "Question 314+85  True 399  Guess 399  Good job\n",
      "Question 191+298 True 489  Guess 488  Fail\n",
      "Question 510+60  True 570  Guess 570  Good job\n",
      "Question 639+52  True 691  Guess 681  Fail\n",
      "Question 502+231 True 733  Guess 633  Fail\n",
      "Question 48+306  True 354  Guess 354  Good job\n",
      "Question 290+207 True 497  Guess 408  Fail\n",
      "Question 425+77  True 502  Guess 402  Fail\n",
      "Question 37+76   True 113  Guess 113  Good job\n",
      "The model scored  45.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 0.4867 - accuracy: 0.8396 - val_loss: 0.4517 - val_accuracy: 0.8500\n",
      "Finished iteration  4\n",
      "Question 872+318 True 1190 Guess 1190 Good job\n",
      "Question 67+122  True 189  Guess 189  Good job\n",
      "Question 6+831   True 837  Guess 837  Good job\n",
      "Question 521+953 True 1474 Guess 1475 Fail\n",
      "Question 16+401  True 417  Guess 417  Good job\n",
      "Question 854+678 True 1532 Guess 1533 Fail\n",
      "Question 448+74  True 522  Guess 523  Fail\n",
      "Question 66+753  True 819  Guess 829  Fail\n",
      "Question 325+518 True 843  Guess 854  Fail\n",
      "Question 7+337   True 344  Guess 344  Good job\n",
      "Question 453+33  True 486  Guess 486  Good job\n",
      "Question 65+28   True 93   Guess 92   Fail\n",
      "Question 93+924  True 1017 Guess 1017 Good job\n",
      "Question 111+50  True 161  Guess 161  Good job\n",
      "Question 475+266 True 741  Guess 722  Fail\n",
      "Question 44+88   True 132  Guess 132  Good job\n",
      "Question 20+963  True 983  Guess 984  Fail\n",
      "Question 632+92  True 724  Guess 724  Good job\n",
      "Question 37+464  True 501  Guess 501  Good job\n",
      "Question 689+712 True 1401 Guess 1401 Good job\n",
      "The model scored  60.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 0.4016 - accuracy: 0.8762 - val_loss: 0.3735 - val_accuracy: 0.8840\n",
      "Finished iteration  5\n",
      "Question 6+321   True 327  Guess 326  Fail\n",
      "Question 952+18  True 970  Guess 970  Good job\n",
      "Question 28+249  True 277  Guess 277  Good job\n",
      "Question 422+253 True 675  Guess 665  Fail\n",
      "Question 83+772  True 855  Guess 855  Good job\n",
      "Question 52+578  True 630  Guess 630  Good job\n",
      "Question 214+93  True 307  Guess 307  Good job\n",
      "Question 341+2   True 343  Guess 343  Good job\n",
      "Question 108+51  True 159  Guess 168  Fail\n",
      "Question 63+889  True 952  Guess 952  Good job\n",
      "Question 398+97  True 495  Guess 495  Good job\n",
      "Question 544+69  True 613  Guess 613  Good job\n",
      "Question 606+945 True 1551 Guess 1551 Good job\n",
      "Question 598+792 True 1390 Guess 1391 Fail\n",
      "Question 50+362  True 412  Guess 412  Good job\n",
      "Question 45+166  True 211  Guess 211  Good job\n",
      "Question 56+97   True 153  Guess 153  Good job\n",
      "Question 965+708 True 1673 Guess 1673 Good job\n",
      "Question 903+52  True 955  Guess 955  Good job\n",
      "Question 41+506  True 547  Guess 547  Good job\n",
      "The model scored  80.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 0.3270 - accuracy: 0.9058 - val_loss: 0.3343 - val_accuracy: 0.8916\n",
      "Finished iteration  6\n",
      "Question 0+293   True 293  Guess 293  Good job\n",
      "Question 795+20  True 815  Guess 815  Good job\n",
      "Question 66+17   True 83   Guess 83   Good job\n",
      "Question 713+340 True 1053 Guess 1033 Fail\n",
      "Question 14+762  True 776  Guess 775  Fail\n",
      "Question 48+927  True 975  Guess 975  Good job\n",
      "Question 44+49   True 93   Guess 93   Good job\n",
      "Question 985+303 True 1288 Guess 1298 Fail\n",
      "Question 933+63  True 996  Guess 996  Good job\n",
      "Question 2+530   True 532  Guess 532  Good job\n",
      "Question 95+28   True 123  Guess 123  Good job\n",
      "Question 114+4   True 118  Guess 118  Good job\n",
      "Question 360+175 True 535  Guess 505  Fail\n",
      "Question 39+111  True 150  Guess 150  Good job\n",
      "Question 384+12  True 396  Guess 395  Fail\n",
      "Question 418+824 True 1242 Guess 1232 Fail\n",
      "Question 3+655   True 658  Guess 658  Good job\n",
      "Question 854+0   True 854  Guess 854  Good job\n",
      "Question 22+743  True 765  Guess 765  Good job\n",
      "Question 54+9    True 63   Guess 63   Good job\n",
      "The model scored  70.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.2694 - accuracy: 0.9259 - val_loss: 0.2591 - val_accuracy: 0.9269\n",
      "Finished iteration  7\n",
      "Question 7+159   True 166  Guess 166  Good job\n",
      "Question 21+548  True 569  Guess 579  Fail\n",
      "Question 872+318 True 1190 Guess 1190 Good job\n",
      "Question 940+34  True 974  Guess 984  Fail\n",
      "Question 932+332 True 1264 Guess 1254 Fail\n",
      "Question 12+202  True 214  Guess 214  Good job\n",
      "Question 67+82   True 149  Guess 149  Good job\n",
      "Question 337+681 True 1018 Guess 1008 Fail\n",
      "Question 42+755  True 797  Guess 797  Good job\n",
      "Question 851+36  True 887  Guess 887  Good job\n",
      "Question 531+17  True 548  Guess 548  Good job\n",
      "Question 645+4   True 649  Guess 659  Fail\n",
      "Question 49+222  True 271  Guess 271  Good job\n",
      "Question 191+298 True 489  Guess 489  Good job\n",
      "Question 51+118  True 169  Guess 179  Fail\n",
      "Question 90+287  True 377  Guess 377  Good job\n",
      "Question 796+637 True 1433 Guess 1432 Fail\n",
      "Question 13+120  True 133  Guess 133  Good job\n",
      "Question 847+5   True 852  Guess 852  Good job\n",
      "Question 820+84  True 904  Guess 904  Good job\n",
      "The model scored  65.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.2269 - accuracy: 0.9381 - val_loss: 0.2146 - val_accuracy: 0.9394\n",
      "Finished iteration  8\n",
      "Question 46+931  True 977  Guess 977  Good job\n",
      "Question 194+53  True 247  Guess 247  Good job\n",
      "Question 983+0   True 983  Guess 983  Good job\n",
      "Question 453+95  True 548  Guess 548  Good job\n",
      "Question 322+620 True 942  Guess 942  Good job\n",
      "Question 13+92   True 105  Guess 105  Good job\n",
      "Question 6+802   True 808  Guess 808  Good job\n",
      "Question 811+278 True 1089 Guess 1099 Fail\n",
      "Question 932+332 True 1264 Guess 1274 Fail\n",
      "Question 839+611 True 1450 Guess 1441 Fail\n",
      "Question 78+31   True 109  Guess 109  Good job\n",
      "Question 134+89  True 223  Guess 223  Good job\n",
      "Question 9+543   True 552  Guess 552  Good job\n",
      "Question 491+917 True 1408 Guess 1408 Good job\n",
      "Question 16+20   True 36   Guess 36   Good job\n",
      "Question 6+23    True 29   Guess 29   Good job\n",
      "Question 412+56  True 468  Guess 468  Good job\n",
      "Question 751+572 True 1323 Guess 1323 Good job\n",
      "Question 98+47   True 145  Guess 145  Good job\n",
      "Question 66+865  True 931  Guess 931  Good job\n",
      "The model scored  85.0  % in its test.\n",
      "\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 0.1882 - accuracy: 0.9509 - val_loss: 0.1976 - val_accuracy: 0.9402\n",
      "Finished iteration  9\n",
      "Question 337+681 True 1018 Guess 1008 Fail\n",
      "Question 711+94  True 805  Guess 805  Good job\n",
      "Question 945+9   True 954  Guess 954  Good job\n",
      "Question 304+93  True 397  Guess 497  Fail\n",
      "Question 91+22   True 113  Guess 113  Good job\n",
      "Question 601+956 True 1557 Guess 1567 Fail\n",
      "Question 0+656   True 656  Guess 656  Good job\n",
      "Question 905+760 True 1665 Guess 1665 Good job\n",
      "Question 433+95  True 528  Guess 528  Good job\n",
      "Question 99+238  True 337  Guess 337  Good job\n",
      "Question 153+325 True 478  Guess 578  Fail\n",
      "Question 512+460 True 972  Guess 972  Good job\n",
      "Question 24+514  True 538  Guess 538  Good job\n",
      "Question 981+987 True 1968 Guess 1978 Fail\n",
      "Question 542+904 True 1446 Guess 1456 Fail\n",
      "Question 60+995  True 1055 Guess 1055 Good job\n",
      "Question 52+400  True 452  Guess 452  Good job\n",
      "Question 551+69  True 620  Guess 620  Good job\n",
      "Question 549+314 True 863  Guess 863  Good job\n",
      "Question 67+4    True 71   Guess 71   Good job\n",
      "The model scored  70.0  % in its test.\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 10):\n",
    "    print()  \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so\n",
    "    # we can visualize errors.\n",
    "    print('Finished iteration ', iteration)\n",
    "    numcorrect = 0\n",
    "    numtotal = 20\n",
    "    \n",
    "    for i in range(numtotal):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=True)\n",
    "        print('Question', q, end=' ')\n",
    "        print('True', correct, end=' ')\n",
    "        print('Guess', guess, end=' ')\n",
    "        if guess == correct :\n",
    "          print('Good job')\n",
    "          numcorrect += 1\n",
    "        else:\n",
    "          print('Fail')\n",
    "    print('The model scored ', numcorrect*100/numtotal,' % in its test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE\n",
    "\n",
    " * Try changing the hyperparams, use other RNNs, more layers, check if increasing the number of epochs is useful.\n",
    "\n",
    " * Try reversing the data from validation set and check if commutative property of addition is learned by the model.\n",
    " * Try printing the hidden layer with two inputs that are commutative and check if the hidden representations it learned are same or similar. Do we expect it to be true? If so, why? If not why? You can access the layer using an index with model.layers and layer.output will give the output of that layer.\n",
    "\n",
    "* Try doing addition in the RNN model the same way we do by hand. Reverse the order of digits and at each time step, input two digits get an output use the hidden layer and input next two digits and so on.(units in the first time step, tens in the second time step etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
